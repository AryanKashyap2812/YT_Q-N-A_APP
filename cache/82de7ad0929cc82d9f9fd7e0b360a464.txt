[ENGLISH TRANSCRIPT]
 When building GenAI applications, retrieval augmented generation is often contrasted with fine tuning as two separate techniques for incorporating domain-specific data into LLM output. Retrieval augmented fine tuning is a hybrid approach that combines the best of both worlds and addresses many of the challenges surrounding LLM performance in specialized settings. Originally developed by researchers at UC Berkeley RAF uses a unique fine tuning technique to improve RAG performance in specific domain contexts. Now with traditional RAG, we provide context to the model during inference by using a retriever to search for relevant documents in a vector database that we append to our prompt, that we send to our LLM. With fine tuning, we provide context to the model during training time by using a large label dataset to bake specific knowledge into a pre-trained LLM. So how can we combine both of these techniques to create retrieval augmented fine tuning or RAF? Let's use an analogy. Let's say that using an LLM on enterprise-specific tasks is like studying for an exam. Suppose that fine tuning is like studying for a closed book exam. Since you can't use your notes, you have to memorize all the materials in advance and if you study all the wrong stuff, you probably won't do so well since you don't have access to new information. In the same way with fine tuning, the model has to rely completely on the knowledge it learned during training in order to answer the user's question. Now RAG would be like taking an open book exam that you did not study for. Because you knew you could use the book on exam day, you chose to skip all the lectures and not read the textbook. So on test day, even though you have all the materials in front of you, there's still no guarantee that you'll actually be able to know where to find all the information. In the same way with RAG, the performance of the the model is largely dictated by how well the retriever can pull relevant documents from the database. Now with RAFT, this is like taking an open book exam that you did study for. This is the win-win situation where you paid attention in all the lectures, read all the materials and get to use the book on the test. So RAFT is similar in that it teaches the model how to use RAG or how to use Excel. So RAFT is a way to teach the model how to use Excel to generate an answer. It's like the saying that goes, give a man a fish and you feed him for a day, but teach a man a fish and you feed him for a lifetime. In the same way, RAFT essentially teaches the model how to fish or how to look for and generate an answer versus just giving it fish or giving it an answer. To explain this more, let's dive into the implementation. Since RAFT is a training technique, we need training data. Each data point will consist of three things, a query, a set of documents, and an answer. Let's look at an example. Let's say our query is how much parental leave does IBM offer? To generate an answer, we can search through two types of documents, core documents and tangent documents. Core documents contain information that's relevant to the user query. In our example, these could be documents on say, pay leave or benefit eligibility. Tension documents, on the other hand, contain information that's irrelevant or off topic to the user query. These could be documents on say, retirement accounts or internal code documentation. From here, we create two types of document sets. Set one contains both core and tangent documents and set two contains just tangent documents. The reason why we include both is to simulate a real RAG use case where the retriever may or may not pull any relevant documents from the database. Finally, to generate our answer, we use chain of thought reasoning to teach the model how to filter past tangent documents and focus on and process through core ones step by step in order to generate a correct answer. We can use this framework to create a larger training data set that we can use to train the model using supervised fine tuning. Now, because this framework is so adaptable, we can use a wide variety of different models and fine tuning techniques to actually implement this in practice. And with that, our model is now ready to ace the exam. So there are three aspects of this training process that I want to highlight that are key to making this whole thing work. One, the inclusion of tangent documents helps to teach the model how to pick out relevant documents from irrelevant ones, thus helping to increase accuracy on domain specific questions. Secondly, the creation of document sets that don't include any relevant documents at all, aka set two, help to teach the model when to rely on its intrinsic knowledge or to say I don't know, versus forcing an incorrect answer out of irrelevant RAG documents. This helps to minimize hallucinations. Third, guiding the model using chain of thought reasoning helps to minimize overfitting and increase transparency and traceability by encouraging the model to quote specific documents from which it got the answer from. So as you can see, RAF creates a model that's both highly scalable and highly robust for enterprise tasks. So whether you found this video because you're studying for that closed book exam or you're just curious about AI, I hope you learned something and enjoyed the video. Thanks for watching. Peace.